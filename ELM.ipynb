{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic elm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELM(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # default_args\n",
    "        self.args = {\n",
    "            'input_dim': 32,\n",
    "            'output_dim': 24,\n",
    "            'hidden_dim': 8,\n",
    "            'activation_fun': 'sigmoid'\n",
    "        }\n",
    "\n",
    "        self.args.update(kwargs) # 更新默认参数\n",
    "\n",
    "        self.activation_fun = getattr(F,self.args[\"activation_fun\"])\n",
    "\n",
    "        self.fc1 = nn.Linear(self.args['input_dim'], self.args['hidden_dim'])\n",
    "        self.fc2 = nn.Linear(self.args['hidden_dim'], self.args['output_dim'], bias=False) # ELM输出层不需要偏置\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():   # 不需要反向传播\n",
    "            x = self.activation_fun(self.fc1(x))\n",
    "            return self.fc2(x)\n",
    "        \n",
    "    def fit(self, data, ground_truth):\n",
    "        # 给定N条data和对应ground_truth，更新fc2的参数\n",
    "        \n",
    "        with torch.no_grad():   # 不需要反向传播\n",
    "            hidden_mat = self.activation_fun(self.fc1(data))\n",
    "            beta = torch.matmul(torch.pinverse(hidden_mat), ground_truth)  # 计算H的广义逆，求beta\n",
    "            self.fc2.weight = nn.Parameter(beta)    # 更新fc2的参数为beta\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理svc数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_SVC_data(file_path):\n",
    "    df = pd.read_excel(file_path, header=None)\n",
    "    data = torch.tensor(df.iloc[2:].values.astype(float), dtype=torch.float32)\n",
    "    target = torch.tensor(df.iloc[1].values.astype(float), dtype=torch.float32).unsqueeze(1)\n",
    "    return data.transpose(0,1), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = read_SVC_data(\"C:\\\\Users\\\\29147\\\\Desktop\\\\近期使用\\\\测试 的副本.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 973])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ELM(input_dim=973, hidden_dim=1300, output_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\29147\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "model.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\29147\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1300 and 1x1300)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28260\\3828582209.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\29147\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28260\\2278689830.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m   \u001b[1;31m# 不需要反向传播\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\29147\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\29147\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\29147\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1751\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1753\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1300 and 1x1300)"
     ]
    }
   ],
   "source": [
    "model(data[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12.4750, 12.0750, 12.1650, 12.3750, 12.1650, 12.3000, 11.8750, 11.9850,\n",
       "         12.0200, 12.1000, 11.8550, 11.9450, 12.0900, 11.9300, 12.0900, 11.8600,\n",
       "         12.1200, 11.8250, 11.7200, 11.7850, 11.9150, 11.7700, 11.8550, 11.9400,\n",
       "         11.8600, 11.8300, 11.7600, 11.7100, 11.7250, 11.6950, 11.6900, 11.6750,\n",
       "         11.7150, 11.7350, 11.6550, 11.7250, 11.6850, 11.7500, 11.6700, 11.7000,\n",
       "         11.7150, 11.7500, 11.6900, 11.6600, 11.6800, 11.6950, 11.7050, 11.7400,\n",
       "         11.7350, 11.7300, 11.7450, 11.7050, 11.7050, 11.7450, 11.7050, 11.7000,\n",
       "         11.7500, 11.7350, 11.7550, 11.7150, 11.7050, 11.7050, 11.7200, 11.6950,\n",
       "         11.6950, 11.6900, 11.6950, 11.7000, 11.6850, 11.6500, 11.6750, 11.6550,\n",
       "         11.6500, 11.6350, 11.6400, 11.6450, 11.6600, 11.6550, 11.6350, 11.6200,\n",
       "         11.6150, 11.6250, 11.6250, 11.6200, 11.6250, 11.6000, 11.6050, 11.5900,\n",
       "         11.5950, 11.6000, 11.5900, 11.5900, 11.6100, 11.6100, 11.6200, 11.6050,\n",
       "         11.5900, 11.5950, 11.5900, 11.6050, 11.6050, 11.6050, 11.5950, 11.6000,\n",
       "         11.6000, 11.6100, 11.6100, 11.6100, 11.6050, 11.6000, 11.6100, 11.6000,\n",
       "         11.6050, 11.6050, 11.6100, 11.6250, 11.6350, 11.6300, 11.6450, 11.6550,\n",
       "         11.6600, 11.6600, 11.6750, 11.6700, 11.6900, 11.7000, 11.7100, 11.7200,\n",
       "         11.7450, 11.7550, 11.7650, 11.7700, 11.7900, 11.8050, 11.8250, 11.8400,\n",
       "         11.8600, 11.8850, 11.9100, 11.9400, 11.9700, 12.0000, 12.0300, 12.0700,\n",
       "         12.1150, 12.1500, 12.2000, 12.2500, 12.3050, 12.3600, 12.4200, 12.4850,\n",
       "         12.5550, 12.6200, 12.6850, 12.7600, 12.8350, 12.9100, 12.9850, 13.0650,\n",
       "         13.1450, 13.2250, 13.3150, 13.4050, 13.4850, 13.5650, 13.6450, 13.7250,\n",
       "         13.7950, 13.8800, 13.9600, 14.0400, 14.1100, 14.1850, 14.2550, 14.3200,\n",
       "         14.3800, 14.4350, 14.4950, 14.5450, 14.5950, 14.6450, 14.6950, 14.7350,\n",
       "         14.7850, 14.8250, 14.8700, 14.9050, 14.9350, 14.9550, 14.9850, 15.0050,\n",
       "         15.0200, 15.0050, 15.0000, 15.0150, 15.0300, 15.0750, 15.1300, 15.1650,\n",
       "         15.2000, 15.2400, 15.2650, 15.3200, 15.3800, 15.4300, 15.4850, 15.5100,\n",
       "         15.5500, 15.5650, 15.5750, 15.5850, 15.5700, 15.5550, 15.5750, 15.6100,\n",
       "         15.6650, 15.7050, 15.7350, 15.7650, 15.8000, 15.8300, 15.8700, 15.9000,\n",
       "         15.9400, 15.9800, 16.0150, 16.0550, 16.0950, 16.1400, 16.1850, 16.2250,\n",
       "         16.2700, 16.3200, 16.3650, 16.4150, 16.4600, 16.5100, 16.5600, 16.6150,\n",
       "         16.6550, 16.7200, 16.7650, 16.8150, 16.8600, 16.9100, 16.9550, 17.0050,\n",
       "         17.0550, 17.1050, 17.1600, 17.2000, 17.2550, 17.3000, 17.3400, 17.3800,\n",
       "         17.4300, 17.4850, 17.5250, 17.5700, 17.6050, 17.6450, 17.6750, 17.7050,\n",
       "         17.7450, 17.7700, 17.8000, 17.8200, 17.8350, 17.8600, 17.8850, 17.8900,\n",
       "         17.9200, 17.9250, 17.9250, 17.9350, 17.9350, 17.9500, 17.9450, 17.9400,\n",
       "         17.9350, 17.9300, 17.9200, 17.9150, 17.8950, 17.8750, 17.8550, 17.8450,\n",
       "         17.8250, 17.8050, 17.7850, 17.7600, 17.7300, 17.7050, 17.6700, 17.6450,\n",
       "         17.6150, 17.6000, 17.5650, 17.5250, 17.5050, 17.4650, 17.4350, 17.4050,\n",
       "         17.3700, 17.3300, 17.3050, 17.2650, 17.2350, 17.1950, 17.1600, 17.1250,\n",
       "         17.0950, 17.0550, 17.0350, 17.0150, 16.9850, 16.9400, 16.9000, 16.8750,\n",
       "         16.8550, 16.8150, 16.7850, 16.7600, 16.7250, 16.7050, 16.6700, 16.6550,\n",
       "         16.6300, 16.6100, 16.5800, 16.5600, 16.5350, 16.5150, 16.4800, 16.4550,\n",
       "         16.4350, 16.4200, 16.3900, 16.3700, 16.3500, 16.3250, 16.3050, 16.2900,\n",
       "         16.2750, 16.2600, 16.2450, 16.2300, 16.2100, 16.1900, 16.1750, 16.1650,\n",
       "         16.1450, 16.1450, 16.1250, 16.1100, 16.1100, 16.0900, 16.0900, 16.0900,\n",
       "         16.0750, 16.0650, 16.0650, 16.0500, 16.0500, 16.0350, 16.0350, 16.0250,\n",
       "         16.0300, 16.0250, 16.0050, 16.0100, 16.0000, 16.0200, 16.0300, 16.0100,\n",
       "         16.0100, 16.0350, 16.0200, 16.0300, 16.0150, 16.0200, 16.0350, 16.0450,\n",
       "         16.0250, 16.0300, 16.0550, 16.0550, 16.0700, 16.0550, 16.0850, 16.0700,\n",
       "         16.0800, 16.1050, 16.0900, 16.1050, 16.1250, 16.1250, 16.1200, 16.1450,\n",
       "         16.1650, 16.1700, 16.1900, 16.2200, 16.2250, 16.2500, 16.2750, 16.2900,\n",
       "         16.3050, 16.3150, 16.3150, 16.3300, 16.3450, 16.3700, 16.3900, 16.3900,\n",
       "         16.4200, 16.4350, 16.4400, 16.4750, 16.4950, 16.5250, 16.5550, 16.5800,\n",
       "         16.6050, 16.6350, 16.6450, 16.6700, 16.7100, 16.7150, 16.7450, 16.7750,\n",
       "         16.8150, 16.8400, 16.8700, 16.8900, 16.9300, 16.9550, 16.9900, 17.0400,\n",
       "         17.0600, 17.0750, 17.1250, 17.1450, 17.2150, 17.2250, 17.2550, 17.2900,\n",
       "         17.3350, 17.3850, 17.4600, 17.4550, 17.5150, 17.5350, 17.5550, 17.6150,\n",
       "         17.6200, 17.7000, 17.7400, 17.7450, 17.7700, 17.8550, 17.8850, 17.9400,\n",
       "         18.0200, 18.0150, 18.0450, 18.0850, 18.1300, 18.1800, 18.2350, 18.2250,\n",
       "         18.3250, 18.3200, 18.3900, 18.4400, 18.4900, 18.5550, 18.6000, 18.6100,\n",
       "         18.6450, 18.6850, 19.6600, 19.6100, 19.6450, 19.6850, 19.7450, 19.8700,\n",
       "         20.0600, 20.2200, 20.3800, 20.5900, 20.7700, 20.9150, 21.0650, 21.2150,\n",
       "         21.3400, 21.4950, 21.6550, 21.7950, 21.9200, 22.0300, 22.1350, 22.2400,\n",
       "         22.3850, 22.5350, 22.6650, 22.7650, 22.8900, 22.9600, 23.0700, 23.2250,\n",
       "         23.3050, 23.4300, 23.5450, 23.6900, 23.8350, 23.9300, 23.9850, 24.0950,\n",
       "         24.2700, 24.3950, 24.5100, 24.6150, 24.7000, 24.7950, 24.9150, 25.0200,\n",
       "         25.1000, 25.2250, 25.3400, 25.3950, 25.4900, 25.5700, 25.6400, 25.6700,\n",
       "         25.7150, 25.8250, 25.8700, 25.9100, 25.9500, 25.9800, 25.9950, 26.0550,\n",
       "         26.1150, 26.1200, 26.1450, 26.1650, 26.1550, 26.1950, 26.2250, 26.2050,\n",
       "         26.1700, 26.1550, 26.2200, 26.2350, 26.2650, 26.2250, 26.2200, 26.2850,\n",
       "         26.2550, 26.2400, 26.2900, 26.2950, 26.3150, 26.3150, 26.3000, 26.3100,\n",
       "         26.2800, 26.2850, 26.2800, 26.3050, 26.3250, 26.3400, 26.3200, 26.3200,\n",
       "         26.3500, 26.3500, 26.3400, 26.3500, 26.3750, 26.3800, 26.3650, 26.3750,\n",
       "         26.3750, 26.3750, 26.3600, 26.3450, 26.3850, 26.3500, 26.3750, 26.4100,\n",
       "         26.3700, 26.3900, 26.4150, 26.4000, 26.4250, 26.4400, 26.4500, 26.4600,\n",
       "         26.4250, 26.4750, 26.4700, 26.4400, 26.4500, 26.4400, 26.4600, 26.4250,\n",
       "         26.4350, 26.4600, 26.4350, 26.4150, 26.4150, 26.4100, 26.4500, 26.4750,\n",
       "         26.4550, 26.4600, 26.4700, 26.4600, 26.4650, 26.4800, 26.4850, 26.5050,\n",
       "         26.5100, 26.5100, 26.4800, 26.4800, 26.5100, 26.5200, 26.5050, 26.4750,\n",
       "         26.4350, 26.4750, 26.4850, 26.4500, 26.4850, 26.4700, 26.4800, 26.4550,\n",
       "         26.5050, 26.5450, 26.5000, 26.4750, 26.4550, 26.4600, 26.5100, 26.4950,\n",
       "         26.4750, 26.5300, 26.5250, 26.5050, 26.5250, 26.4750, 26.4550, 26.5350,\n",
       "         26.5400, 26.5500, 26.5450, 26.5800, 26.5950, 26.5350, 26.5450, 26.5450,\n",
       "         26.5650, 26.6050, 26.6250, 26.6600, 26.6600, 26.6200, 26.5950, 26.6250,\n",
       "         26.6300, 26.6350, 26.6850, 26.6750, 26.6600, 26.6950, 26.6900, 26.6900,\n",
       "         26.6700, 26.6150, 26.6100, 26.6050, 26.6050, 26.6250, 26.6450, 26.6200,\n",
       "         26.6450, 26.6850, 26.6800, 26.6850, 26.6500, 26.6600, 26.6500, 26.6300,\n",
       "         26.6600, 26.6200, 26.6200, 26.6400, 26.6050, 26.5800, 26.6200, 26.6500,\n",
       "         26.6550, 26.6500, 26.6350, 26.6900, 26.6900, 26.6950, 26.7300, 26.6800,\n",
       "         26.6800, 26.7350, 26.7750, 26.7500, 26.7200, 26.7350, 26.7300, 26.7150,\n",
       "         26.7050, 26.7300, 26.8200, 26.9000, 26.8550, 26.7250, 26.5900, 26.5900,\n",
       "         26.5700, 26.5800, 26.4900, 27.0950, 27.0550, 27.0400, 27.0300, 26.9550,\n",
       "         26.9300, 26.9200, 26.8550, 26.8350, 26.9000, 26.8550, 26.8850, 26.9300,\n",
       "         26.9100, 26.9000, 26.9150, 26.9150, 26.9050, 26.9500, 26.9000, 26.9450,\n",
       "         27.0350, 26.9850, 27.0300, 27.1000, 27.0750, 27.0900, 27.1400, 27.1500,\n",
       "         27.1700, 27.2050, 27.2000, 27.2750, 27.3250, 27.3050, 27.3100, 27.4150,\n",
       "         27.4050, 27.4000, 27.4700, 27.4700, 27.5300, 27.5700, 27.5600, 27.6000,\n",
       "         27.6450, 27.6600, 27.7200, 27.7800, 27.7400, 27.8300, 27.8750, 27.8150,\n",
       "         27.8350, 27.9000, 27.9700, 27.9550, 27.9700, 27.9700, 27.9800, 28.0250,\n",
       "         28.0450, 28.0350, 28.1200, 28.1350, 28.1050, 28.1500, 28.1450, 28.1850,\n",
       "         28.2800, 28.2400, 28.2450, 28.3150, 28.3100, 28.3650, 28.3900, 28.4150,\n",
       "         28.4300, 28.4750, 28.5000, 28.5300, 28.5550, 28.5700, 28.6450, 28.6350,\n",
       "         28.6150, 28.6350, 28.6650, 28.6750, 28.5950, 28.6250, 28.6300, 28.6350,\n",
       "         28.6100, 28.5050, 28.5050, 28.5000, 28.4400, 28.3650, 28.3400, 28.3000,\n",
       "         28.2200, 28.1800, 28.1400, 28.0850, 28.1050, 28.0900, 27.9650, 27.9650,\n",
       "         27.8950, 27.8350, 27.9350, 27.8100, 27.7050, 27.8000, 27.6850, 27.6450,\n",
       "         27.7250, 27.6900, 27.7050, 27.7300, 27.6500, 27.6500, 27.6800, 27.6050,\n",
       "         27.6600, 27.7050, 27.6550, 27.6500, 27.6800, 27.6400, 27.6350, 27.7100,\n",
       "         27.6750, 27.6700, 27.7450, 27.7150, 27.7300, 27.7750, 27.6900, 27.7200,\n",
       "         27.8500, 27.7500, 27.6400, 27.7900, 27.8600, 27.7900, 27.8900, 27.9200,\n",
       "         27.8950, 27.9300, 27.8800, 27.9350, 27.9900, 27.9250, 27.9500, 28.0400,\n",
       "         28.0050, 27.9650, 28.0300, 28.0800, 28.0000, 28.0800, 28.0800, 28.0750,\n",
       "         28.1800, 28.1050, 28.2050, 28.2500, 28.1900, 28.2700, 28.2900, 28.2700,\n",
       "         28.3250, 28.3350, 28.3100, 28.4550, 28.3550, 28.3450, 28.4250, 28.3650,\n",
       "         28.3950, 28.4750, 28.4950, 28.3250, 28.4000, 28.5850, 28.4600, 28.5200,\n",
       "         28.6450, 28.4750, 28.4100, 28.5000, 28.5000, 28.5150, 28.4900, 28.5050,\n",
       "         28.6300, 28.5350, 28.4750, 28.5550, 28.6950, 28.4900, 28.5400, 28.7150,\n",
       "         28.3850, 28.3950, 28.5700, 28.5000, 28.4850, 28.5550, 28.5700, 28.5000,\n",
       "         28.6700, 28.4300, 28.3500, 28.5950, 28.3650, 28.4050, 28.6150, 28.5400,\n",
       "         28.4750, 28.5150, 28.5400, 28.6900, 28.6000, 28.3100, 28.4650, 28.7250,\n",
       "         28.4000, 28.2250, 28.5600, 28.5400, 28.5250, 28.5050, 28.2850, 28.6100,\n",
       "         28.7600, 28.5500, 28.7000, 28.6200, 28.5450]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].unsqueeze(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
